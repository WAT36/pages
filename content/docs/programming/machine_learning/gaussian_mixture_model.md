---
title: "混合ガウスモデル"
weight: 1
# bookFlatSection: false
# bookShowToC: true
bookToc: false
---

# 混合ガウスモデル

先程のk-means法では、データを必ずどれか一つのクラスターに割り当てていたが、複数のクラスターに割り当てると言うことはできないだろうか？

ここで、教師あり学習でも利用した、確率を使った表現を利用することを考えてみる。

k-means法では入力データがどのクラスタに属するかを示すものとして行列Rを用いていた。

ここでは、この行列Rを、以下のように行列Γとして設定する。なお、入力データの個数をN個とする。

{{< katex  >}}
\tag{1}  {\bf \Gamma}  =  
                \left[
                    \begin{array}{cc}
                    {\bf \gamma}_{0} \\
                    {\bf \gamma}_{1} \\
                    \vdots \\
                    {\bf \gamma}_{N-1} \\
                    \end{array}
                \right]
{{< /katex >}}

ここでクラスタ数をKとした時、

{{< katex  >}}
\tag{2}  {\bf \gamma}_{i}  =  
                \left[
                    \begin{array}{cc}
                    \gamma_{i0}  & \gamma_{i1} & \cdots & \gamma_{i(K-1)}
                    \end{array}
                \right]
{{< /katex >}}

となる。γ<sub>ik</sub>は、i番目の入力データがクラスタkに属する確率を示す。

また、これより

{{< katex  >}}
\tag{3}  \sum_{k=0}^{K-1} \gamma_{ik}  =  1
{{< /katex >}}

である。

## データがクラスタに属する確率とは？

そもそもの話だが、入力データがクラスタkに属する確率とはどう言う意味なのか？

具体的な例を使って説明してみよう。先程の例では温度とpHを入力としたデータを利用していた。

ここで、例えば温泉の源泉が複数あり、それらの組み合わさったことによって入力データの温度とpHが実現している、と言う説を考えてみよう。

そうなると、元となる源泉の温度とpHは固定されているので、それらが各クラスタの中心をなしていると考えられ、かつそのようなクラスタが存在しているとも考えることもできる。

このように、観察はできなかったがデータに影響を与えているだろう変量のことを**潜在変数(latent valiable)**または**隠れ変数(hidden variable)**と呼ぶ。


この潜在変数を数式で定義するにはどうすれば良いだろうか。(2)と同様にして、潜在変数の個数をK個とした時、以下のようなz<sub>i</sub>を考えてみる。(0≦i＜N)

{{< katex  >}}
\tag{4}  {\bf z }_{i}  =  
                \left[
                    \begin{array}{cc}
                    z_{i0}  & z_{i1} & \cdots & z_{i(K-1)}
                    \end{array}
                \right]
{{< /katex >}}

式(4)において、z<sub>ij</sub>は、i番目のデータがj番目の潜在変数に属していたら１、属していなければ０をとる変数とする。(0≦j＜K)

これより、γ<sub>ik</sub>（i番目の入力データがクラスタkに属する確率）は、i番目の入力データが潜在変数kに属する確率と言い換えることもでき、以下のような式で表される。

{{< katex  >}}
\tag{5}  {\bf \gamma}_{ik}   =  P(z_{ik} = 1 | x_{i})
{{< /katex >}}

式(5)は、簡単にいうとz<sub>ik</sub>の値がわからない時に、z<sub>ik</sub>の推定値として利用できる。入力データx<sub>i</sub>がどの潜在変数に属するかわかっていれば、式(5)の値は0または1になるので推定はしなくても良いが、x<sub>i</sub>がどの潜在変数に属するかがわからない場合はこの式(5)の値が0から1の値を取るので、この値が潜在変数に属する確率としてそのまま利用できる。

この式(5)で表されるγは、「どの潜在変数(クラスタ)にどれほど寄与しているか」という意味合いから、**負担率 (responsibility)**とも呼ばれている。

確率的にクラスタリングを行うということは、データの背後に潜む潜在変数Zを確率的な変数γ(負担率)として推定することである。

次に、この負担率γを推定する方法についてを述べる。


## 混合ガウスモデル

負担率γを求めるために、混合ガウスモデルという確率モデルを導入する。

混合ガウスモデルは、２次元ガウス関数を複数足し合わせたもので、以下の式で表される。

{{< katex  >}}
\tag{6}  p( {\bf x} )  =  \sum_{k=0}^{K-1} \pi_{k} N ({\bf x} | {\bf \mu}_{k} , {\bf \Sigma}_{k} )
{{< /katex >}}


N(x|μ<sub>k</sub>,Σ<sub>k</sub>)は平均μ<sub>k</sub>、共分散行列Σ<sub>k</sub>の２次元ガウス関数を表している。

このモデルのパラメータは、各ガウス分布の中心を表す中心ベクトルμ<sub>k</sub>、分布の散らばり具合を示す共分散行列Σ<sub>k</sub>、各ガウス分布の大きさ(係数)を示す**混合係数** π<sub>k</sub>である。混合係数は0から1の間を取る値であり、また全ての混合係数の和は1となる。


{{< katex  >}}
\tag{7}  \sum_{k=0}^{K-1} \pi_{k}  =  1
{{< /katex >}}

では、どのようにして入力データに対する最適なパラメータμ<sub>k</sub>、Σ<sub>k</sub>、π<sub>k</sub>を求めれば良いか。そのための方法を次に示す。