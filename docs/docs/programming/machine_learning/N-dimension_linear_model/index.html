<!DOCTYPE html>
<html lang="en" dir="ltr">

<head>
  <meta name="generator" content="Hugo 0.80.0" />
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="N次元線形回帰モデル # 先程までの章では1次元(直線モデル)、2次元(面モデル)の入力データを扱ってきたが、そこから更に次元を広げたN次元の入">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="N次元線形回帰モデル" />
<meta property="og:description" content="N次元線形回帰モデル # 先程までの章では1次元(直線モデル)、2次元(面モデル)の入力データを扱ってきたが、そこから更に次元を広げたN次元の入" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://WAT36.github.io/pages/docs/programming/machine_learning/N-dimension_linear_model/" />
<meta property="article:modified_time" content="2020-04-25T01:30:38+09:00" />
<title>N次元線形回帰モデル | WAT Notes</title>
<link rel="manifest" href="https://WAT36.github.io/pages/manifest.json">
<link rel="icon" href="https://WAT36.github.io/pages/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="https://WAT36.github.io/pages/book.min.6c7c6446dfdee7c8c933e9bbc6e80ee3ed6c913b2a59519f2092c3c6a9d63e55.css" integrity="sha256-bHxkRt/e58jJM&#43;m7xugO4&#43;1skTsqWVGfIJLDxqnWPlU=">
<script defer src="https://WAT36.github.io/pages/en.search.min.6f67fa14517efb6d8066b649593d40c040c008d6f097ccf352748dee33509bd1.js" integrity="sha256-b2f6FFF&#43;&#43;22AZrZJWT1AwEDACNbwl8zzUnSN7jNQm9E="></script>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-161908250-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a href="https://WAT36.github.io/pages/"><span>WAT Notes</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>











  <ul>
<li><a href="https://WAT36.github.io/pages/docs/about/aboutme/"><strong>About me</strong></a></li>
</ul>
<details>
 <summary>Notes</summary>
<ul>
<li>
<ul>
<li><a href="https://WAT36.github.io/pages/docs/programming/jp_index/"><strong>プログラミング</strong></a></li>
<li><a href="https://WAT36.github.io/pages/docs/ctf/ctf_index/"><strong>CTF</strong></a></li>
<li><a href="https://WAT36.github.io/pages/docs/front-end/front_index/"><strong>フロントエンド</strong></a></li>
<li><a href="https://WAT36.github.io/pages/docs/cloud/aws/aws_index/"><strong>クラウド(AWS)</strong></a></li>
<li><a href="https://WAT36.github.io/pages/docs/container/container_index/"><strong>コンテナ</strong></a></li>
</ul>
</li>
</ul>
</details>
<ul>
<li>
<p><a href="https://WAT36.github.io/pages/posts/"><strong>Blog</strong></a></p>
</li>
<li>
<p><a href="https://WAT36.github.io/pages/docs/about/disclaimer/">免責事項</a></p>
</li>
</ul>










</nav>




  <script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="https://WAT36.github.io/pages/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>N次元線形回帰モデル</strong>

  <label for="toc-control">
    
  </label>
</div>


  
 
      </header>

      
      
  <article class="markdown"><h1 id="n次元線形回帰モデル">
  N次元線形回帰モデル
  <a class="anchor" href="#n%e6%ac%a1%e5%85%83%e7%b7%9a%e5%bd%a2%e5%9b%9e%e5%b8%b0%e3%83%a2%e3%83%87%e3%83%ab">#</a>
</h1>
<p>先程までの章では1次元(直線モデル)、2次元(面モデル)の入力データを扱ってきたが、そこから更に次元を広げたN次元の入力データの場合はどうなるだろうか。
ここではそれについてを述べる。</p>
<p>N次元での入力データから予測値yを算出する式は以下の式で表される。</p>

<link rel="stylesheet" href="https://WAT36.github.io/pages/katex/katex.min.css" />
<script defer src="https://WAT36.github.io/pages/katex/katex.min.js"></script>
<script defer src="https://WAT36.github.io/pages/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \(\tag{1}  y=w_{0} x_{0} &#43; w_{1} x_{1} &#43; w_{2} x_{2} &#43; \cdots &#43; w_{N-1} x_{N-1} &#43; w_{N} (w_{i} は実数)\)
</span>

<p>N次元の時も1次元2次元の時と同様に式(1)の様な形で表される。この式の形で表されるモデルは<strong>線形回帰モデル</strong>と呼ばれている。</p>
<p>式(1)において、最後のw<sub>N</sub>には入力データがない事に注意する。 (切片である)</p>
<p>ここでは簡略化のため、以降w<sub>N</sub> = 0として考える。すると式(1)は以下の様になる。</p>
<span>
  \(\tag{2}  y=w_{0} x_{0} &#43; w_{1} x_{1} &#43; w_{2} x_{2} &#43; \cdots &#43; w_{N-1} x_{N-1} \)
</span>

<p>この式(2)を行列表記で書き直すと、以下の様になる。</p>
<span>
  \(\begin{aligned}
\tag{3}  y  &amp;= w_{0} x_{0} &#43; w_{1} x_{1} &#43; w_{2} x_{2} &#43; \cdots &#43; w_{N-1} x_{N-1} \\
            &amp;=  \left[
                    \begin{array}{ccc}
                    w_{0} &amp; \cdots &amp; w_{N-1} 
                    \end{array}
                \right]
                \left[
                    \begin{array}{cccc}
                    x_{0} \\
                    \vdots \\
                    x_{N-1}
                    \end{array}
                \right]
            \\
            &amp;= {\bf w} ^\mathrm{T} {\bf x} 
\end{aligned}\)
</span>

<p>ここで</p>
<span>
  \(  {\bf w} = \left[
    \begin{array}{cccc}
      w_{0} \\
      w_{1} \\
      \vdots \\
      w_{N-1}
    \end{array}
  \right]
  ,
    {\bf x} = \left[
    \begin{array}{cccc}
      x_{0} \\
      x_{1} \\
      \vdots \\
      x_{N-1}
    \end{array}
  \right]\)
</span>

<p>とおく。</p>
<hr>
<p>では、ここからN次元線形回帰モデルの解析解を求めてみよう。</p>
<p>これまでと同様にして、平均二乗誤差Jは以下の様に表される。</p>
<span>
  \(\begin{aligned}
\tag{4}  J( {\bf w} ) &amp;= \frac{1}{N} \sum_{n=0}^{N-1} ( y(x_{n}) - t_{n} )^2 \\
                      &amp;= \frac{1}{N} \sum_{n=0}^{N-1} ( {\bf w} ^\mathrm{T} {\bf x}_{n} - t_{n} )^2
\end{aligned}\)
</span>

<p>ここも同様にして、式(4)をw<sub>i</sub>で偏微分すると、以下の様になる。</p>
<span>
  \(\begin{aligned}
\tag{5}  \frac{\partial J}{\partial w_{i} } 
            &amp;= \frac{1}{N} \sum_{n=0}^{N-1} \frac{\partial }{\partial w_{i} } ( {\bf w} ^\mathrm{T} {\bf x}_{n} - t_{n} )^2 \\
            &amp;= \frac{2}{N} \sum_{n=0}^{N-1} ( {\bf w} ^\mathrm{T} {\bf x}_{n} - t_{n} ) x_{n,i} 
\end{aligned}\)
</span>

<p>なお、x<sub>n,i</sub>はn番目の入力データにおけるi番目のパラメータである。</p>
<p>また、<b>w</b><sup>T</sup><b>x</b>をw<sub>i</sub>で偏微分すると、x<sub>n,i</sub>だけが残る事に注意する。</p>
<p>Jを最小にする<b>w</b>では、全てのw<sub>i</sub>について傾き0、つまり式(5)が０になるので、次の式が全てのiで成り立つ。</p>
<span>
  \(\begin{aligned}
\tag{6}  \frac{2}{N} \sum_{n=0}^{N-1} ( {\bf w} ^\mathrm{T} {\bf x}_{n} - t_{n} ) x_{n,i} &amp;= 0 \\
\Leftrightarrow  \sum_{n=0}^{N-1} ( {\bf w} ^\mathrm{T} {\bf x}_{n} - t_{n} ) x_{n,i} &amp;= 0
\end{aligned}\)
</span>

<p>式(6)は全てのi (0≦i≦N-1) で成り立つ。つまり以下の式が成り立つ。</p>
<span>
  \(\begin{aligned}
\tag{7} \sum_{n=0}^{N-1} ( {\bf w} ^\mathrm{T} {\bf x}_{n} - t_{n} ) x_{n,0} &amp;= 0 \\
        \sum_{n=0}^{N-1} ( {\bf w} ^\mathrm{T} {\bf x}_{n} - t_{n} ) x_{n,1} &amp;= 0 \\
        \vdots \\
        \sum_{n=0}^{N-1} ( {\bf w} ^\mathrm{T} {\bf x}_{n} - t_{n} ) x_{n,N-1} &amp;= 0 
\end{aligned}\)
</span>

<p>この式をベクトルを使って書き換えると以下の式の様になる。</p>
<span>
  \(\begin{aligned}
\tag{8} &amp;\sum_{n=0}^{N-1} ( {\bf w} ^\mathrm{T} {\bf x}_{n} - t_{n} ) [ x_{n,0}, x_{n,1}, \cdots , x_{n,N-1} ] 
        = 
                \left[
                    \begin{array}{ccc}
                    0 &amp; 0 &amp; \cdots &amp; 0 
                    \end{array}
                \right]  \\
        \Leftrightarrow 
        &amp;\sum_{n=0}^{N-1} ( {\bf w} ^\mathrm{T} {\bf x}_{n} - t_{n} ) {\bf x}_{n} ^\mathrm{T}
        = 
                \left[
                    \begin{array}{ccc}
                    0 &amp; 0 &amp; \cdots &amp; 0 
                    \end{array}
                \right] \\
        \Leftrightarrow 
        &amp;\sum_{n=0}^{N-1} ( {\bf w} ^\mathrm{T} {\bf x}_{n} {\bf x}_{n} ^\mathrm{T} 
                          - t_{n} {\bf x}_{n} ^\mathrm{T} ) 
        = 
                \left[
                    \begin{array}{ccc}
                    0 &amp; 0 &amp; \cdots &amp; 0 
                    \end{array}
                \right] \\
        \Leftrightarrow 
        &amp; {\bf w} ^\mathrm{T} \sum_{n=0}^{N-1} {\bf x}_{n} {\bf x}_{n} ^\mathrm{T} 
          -  \sum_{n=0}^{N-1} t_{n} {\bf x}_{n} ^\mathrm{T}  
        = 
                \left[
                    \begin{array}{ccc}
                    0 &amp; 0 &amp; \cdots &amp; 0 
                    \end{array}
                \right]
\end{aligned}\)
</span>

<p>ここで</p>
<span>
  \(\begin{aligned}
\tag{9} 
         \sum_{n=0}^{N-1} {\bf x}_{n} {\bf x}_{n} ^\mathrm{T} 
        &amp;= 
         \sum_{n=0}^{N-1}
                \left[
                    \begin{array}{cccc}
                    x_{n,0} \\
                    x_{n,1} \\
                    \vdots \\
                    x_{n,N-1}
                    \end{array}
                \right]
                \left[
                    \begin{array}{ccc}
                    x_{n,0} &amp; x_{n,1} &amp; \cdots &amp; x_{n,N-1} 
                    \end{array}
                \right] \\
        &amp;= 
          \sum_{n=0}^{N-1}
                \left[
                    \begin{array}{cccc}
                    x_{n,0}^2 &amp; x_{n,0} x_{n,1} &amp; \cdots &amp; x_{n,0} x_{n,N-1} \\
                    x_{n,1} x_{n,0} &amp; x_{n,1}^2 &amp; \cdots &amp; x_{n,1} x_{n,N-1} \\
                    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                    x_{n,N-1} x_{n,0} &amp; x_{n,N-1} x_{n,1} &amp; \cdots &amp; x_{n,N-1}^2
                    \end{array}
                \right] \\
        &amp;=
                \left[
                    \begin{array}{llll}
                    \displaystyle \sum_{n=0}^{N-1} x_{n,0}^2 &amp; \displaystyle \sum_{n=0}^{N-1} x_{n,0} x_{n,1} &amp; \cdots &amp; \displaystyle \sum_{n=0}^{N-1} x_{n,0} x_{n,N-1} \\
                    \displaystyle \sum_{n=0}^{N-1} x_{n,1} x_{n,0} &amp; \displaystyle \sum_{n=0}^{N-1} x_{n,1}^2 &amp; \cdots &amp; \displaystyle \sum_{n=0}^{N-1} x_{n,1} x_{n,N-1} \\
                    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                    \displaystyle \sum_{n=0}^{N-1} x_{n,N-1} x_{n,0} &amp; \displaystyle \sum_{n=0}^{N-1} x_{n,N-1} x_{n,1} &amp; \cdots &amp; \displaystyle \sum_{n=0}^{N-1} x_{n,N-1}^2
                    \end{array}
                \right] \\
        &amp;=
                \left[
                    \begin{array}{llll}
                    x_{0,0} &amp; x_{1,0} &amp; \cdots &amp; x_{N-1,0} \\
                    x_{0,1} &amp; x_{1,1} &amp; \cdots &amp; x_{N-1,1} \\
                    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                    x_{0,N-1} &amp; x_{1,N-1} &amp; \cdots &amp; x_{N-1,N-1}
                    \end{array}
                \right] 
                \left[
                    \begin{array}{llll}
                    x_{0,0} &amp; x_{0,1} &amp; \cdots &amp; x_{0,N-1} \\
                    x_{1,0} &amp; x_{1,1} &amp; \cdots &amp; x_{1,N-1} \\
                    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                    x_{N-1,0} &amp; x_{N-1,1} &amp; \cdots &amp; x_{N-1,N-1}
                    \end{array}
                \right] \\
        &amp;= {\bf X} ^\mathrm{T} {\bf X}


\end{aligned}\)
</span>

<span>
  \(\begin{aligned}
\tag{10} 
         \sum_{n=0}^{N-1} t_{n} {\bf x}_{n} ^\mathrm{T} 
        &amp;= 
         \sum_{n=0}^{N-1}
                t_{n}
                \left[
                    \begin{array}{ccc}
                    x_{n,0} &amp; x_{n,1} &amp; \cdots &amp; x_{n,N-1} 
                    \end{array}
                \right] \\
        &amp;= 
          \sum_{n=0}^{N-1}
                \left[
                    \begin{array}{ccc}
                    t_{n} x_{n,0} &amp; t_{n} x_{n,1} &amp; \cdots &amp; t_{n} x_{n,N-1} 
                    \end{array}
                \right] \\
        &amp;=
                \left[
                    \begin{array}{llll}
                    \displaystyle \sum_{n=0}^{N-1} t_{n} x_{n,0} &amp; \displaystyle \sum_{n=0}^{N-1} t_{n} x_{n,1} &amp; \cdots &amp; \displaystyle \sum_{n=0}^{N-1} t_{n} x_{n,N-1} 
                    \end{array}
                \right] \\
        &amp;=
                \left[
                    \begin{array}{ccc}
                    t_{0} &amp; t_{1} &amp; \cdots &amp; t_{N-1} 
                    \end{array}
                \right]
                \left[
                    \begin{array}{llll}
                    x_{0,0} &amp; x_{0,1} &amp; \cdots &amp; x_{0,N-1} \\
                    x_{1,0} &amp; x_{1,1} &amp; \cdots &amp; x_{1,N-1} \\
                    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                    x_{N-1,0} &amp; x_{N-1,1} &amp; \cdots &amp; x_{N-1,N-1}
                    \end{array}
                \right] \\
        &amp;= {\bf t} ^\mathrm{T} {\bf X}
\end{aligned}\)
</span>

<p>とおく、ここで</p>
<span>
  \(\tag{11} 
        {\bf t}
        = 
                \left[
                    \begin{array}{cccc}
                    t_{0} \\
                    t_{1} \\
                    \vdots \\
                    t_{N-1}
                    \end{array}
                \right]
        ,
        {\bf X}
        = 
                \left[
                    \begin{array}{cccc}
                    x_{0,0} &amp; x_{0,1} &amp; \cdots &amp; x_{0,N-1} \\
                    x_{1,0} &amp; x_{1,1} &amp; \cdots &amp; x_{1,N-1} \\
                    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                    x_{N-1,0} &amp; x_{N-1,1} &amp; \cdots &amp; x_{N-1,N-1}
                    \end{array}
                \right]\)
</span>

<p>である。</p>
<p>これより、式(8)は以下式(12)の様に書き換えられる。</p>
<span>
  \(\begin{aligned}
\tag{12}  &amp; {\bf w} ^\mathrm{T} \sum_{n=0}^{N-1} {\bf x}_{n} {\bf x}_{n} ^\mathrm{T} 
          -  \sum_{n=0}^{N-1} t_{n} {\bf x}_{n} ^\mathrm{T}  
        &amp;= 
                \left[
                    \begin{array}{ccc}
                    0 &amp; 0 &amp; \cdots &amp; 0 
                    \end{array}
                \right] \\
        \Leftrightarrow 
        &amp; {\bf w} ^\mathrm{T} {\bf X} ^\mathrm{T} {\bf X} -  {\bf t} ^\mathrm{T} {\bf X}
        &amp;= 
                \left[
                    \begin{array}{ccc}
                    0 &amp; 0 &amp; \cdots &amp; 0 
                    \end{array}
                \right] 
\end{aligned}\)
</span>

<p>式(12)で<b>t</b><sup>T</sup><b>X</b>を右辺に移項すると</p>
<span>
  \(\tag{13}  {\bf w} ^\mathrm{T}  {\bf X} ^\mathrm{T} {\bf X} 
        = {\bf t} ^\mathrm{T}  {\bf X} \)
</span>

<p>ここで両辺を転置すると、(<b>AB</b>)<sup>T</sup> = <b>B</b><sup>T</sup><b>A</b><sup>T</sup>より式(13)は</p>
<span>
  \(\begin{aligned}
\tag{14}   ( {\bf w} ^\mathrm{T}  {\bf X} ^\mathrm{T} {\bf X} )^\mathrm{T}
        &amp;= ( {\bf t} ^\mathrm{T}  {\bf X} )^\mathrm{T} \\
           ( {\bf X} ^\mathrm{T} {\bf X} )^\mathrm{T} ({\bf w} ^\mathrm{T}) ^\mathrm{T}
        &amp;= {\bf X} ^\mathrm{T}  {\bf t} \\
           ( {\bf X} ^\mathrm{T} {\bf X} ) {\bf w}
        &amp;= {\bf X} ^\mathrm{T}  {\bf t}
\end{aligned}\)
</span>

<p>となり、この式(14)に左から(<b>X</b><sup>T</sup><b>X</b>)<sup>-1</sup>をかける事により、<b>w</b>は以下の式(15)の様に表される。</p>
<span>
  \(\tag{15}   {\bf w} = ( {\bf X} ^\mathrm{T}  {\bf X} )^{-1} {\bf X} ^\mathrm{T} {\bf t}\)
</span>

<p>よって、長くなったが式(15)により<b>w</b>の値が導出された。またこれがN次元線形回帰モデルの解となる。</p>
<p>そして、式(15)の右辺の式 (<b>X</b><sup>T</sup><b>X</b>)<sup>-1</sup><b>X</b><sup>T</sup>
は<strong>ムーアーペンローズの擬似逆行列</strong>という名が付けられている。</p>
<hr>
<p>さて、式(2)で計算の簡略化のためw<sub>N</sub>=0としたと書いたが、w<sub>N</sub>≠0の場合も勿論ある。その場合<b>w</b>や<b>x</b>の次数が異なるがどうすれば良いのか？</p>
<p>その場合、w<sub>N</sub>にもダミー用のパラメータであるx<sub>N</sub>を追加する。</p>
<p>ここで、常にx<sub>N</sub>=1 となる様に設定する。</p>
<span>
  \(\begin{aligned}
\tag{16}     y &amp;= w_{0} x_{0} &#43; w_{1} x_{1} &#43; w_{2} x_{2} &#43; \cdots &#43; w_{N-1} x_{N-1} &#43; w_{N} \\
           \Leftrightarrow  y &amp;= w_{0} x_{0} &#43; w_{1} x_{1} &#43; w_{2} x_{2} &#43; \cdots &#43; w_{N-1} x_{N-1} &#43; w_{N} x_{N} ( x_{N} = 1)
\end{aligned}\)
</span>

<p>この式(16)に対しムーアーペンローズの擬似逆行列を作成すれば、wが求められる。これによりw<sub>N</sub>≠0の場合においても解が求められた。</p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">



  <div><a class="flex align-center" href="https://github.com/alex-shpak/hugo-book/commit/633321a4ff8631bd0021802e57d0e4a75ac40bc0" title='Last modified by Wataru Tsukagoshi | Apr 25, 2020' target="_blank" rel="noopener">
      <img src="https://WAT36.github.io/pages/svg/calendar.svg" class="book-icon" alt="Calendar" />
      <span>Apr 25, 2020</span>
    </a>
  </div>



  <div>
    <a class="flex align-center" href="https://github.com/alex-shpak/hugo-book/edit/master/exampleSite/content//docs/programming/machine_learning/N-dimension_linear_model.md" target="_blank" rel="noopener">
      <img src="https://WAT36.github.io/pages/svg/edit.svg" class="book-icon" alt="Edit" />
      <span>Edit this page</span>
    </a>
  </div>

</div>

 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
  </main>

  
</body>

</html>












